"""
HDF5 data storage and management for FSOC-PINO datasets.

This module provides utilities for loading, processing, and managing
HDF5 datasets generated by the simulation pipeline.
"""

import numpy as np
from typing import List, Dict, Tuple, Optional, Iterator
from pathlib import Path
import h5py
import json
import torch
from torch.utils.data import Dataset, DataLoader


class HDF5Manager:
    """
    Manager for HDF5 dataset files.
    
    Provides utilities for loading, combining, and processing
    multiple HDF5 dataset files.
    """
    
    def __init__(self, dataset_dir: Path):
        """
        Initialize HDF5 manager.
        
        Args:
            dataset_dir: Directory containing HDF5 dataset files
        """
        self.dataset_dir = Path(dataset_dir)
        self.batch_files = list(self.dataset_dir.glob("batch_*.h5"))
        self.metadata_file = self.dataset_dir / "dataset_metadata.json"
        
        if not self.batch_files:
            raise ValueError(f"No batch files found in {dataset_dir}")
        
        # Load metadata
        self.metadata = self._load_metadata()
        
        # Cache dataset info
        self._cache_dataset_info()
    
    def _load_metadata(self) -> Dict:
        """Load dataset metadata."""
        if self.metadata_file.exists():
            with open(self.metadata_file, 'r') as f:
                return json.load(f)
        else:
            # Create minimal metadata from first file by reading attributes
            with h5py.File(self.batch_files[0], 'r') as f:
                return {
                    'parameter_names': list(f.attrs['parameter_names']),
                    'grid_size': int(f.attrs['grid_size'])
                }
    
    def _cache_dataset_info(self):
        """Cache dataset information."""
        self.total_samples = 0
        self.grid_size = None
        self.num_parameters = None
        self.parameter_names = []
        
        for batch_file in self.batch_files:
            with h5py.File(batch_file, 'r') as f:
                self.total_samples += f['irradiance'].shape[0]
                if self.grid_size is None:
                    self.grid_size = f['irradiance'].shape[1]
                    self.parameter_names = list(f.attrs['parameter_names'])
                    self.num_parameters = len(self.parameter_names)
    
    def get_dataset_info(self) -> Dict:
        """Get dataset information."""
        return {
            'total_samples': self.total_samples,
            'grid_size': self.grid_size,
            'num_parameters': self.num_parameters,
            'num_batch_files': len(self.batch_files),
            'parameter_names': self.metadata.get('parameter_names', []),
            'dataset_dir': str(self.dataset_dir)
        }
    
    def load_all_data(self) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
        """
        Load all data into memory.
        
        Returns:
            Tuple of (parameters, irradiance, field_real, field_imag)
        """
        # Pre-allocate arrays
        all_parameters = np.zeros((self.total_samples, self.num_parameters), dtype=np.float32)
        all_irradiance = np.zeros((self.total_samples, self.grid_size, self.grid_size), dtype=np.float32)
        all_field_real = np.zeros((self.total_samples, self.grid_size, self.grid_size), dtype=np.float32)
        all_field_imag = np.zeros((self.total_samples, self.grid_size, self.grid_size), dtype=np.float32)
        
        # Load data from all batch files
        start_idx = 0
        for batch_file in self.batch_files:
            with h5py.File(batch_file, 'r') as f:
                batch_size = f['irradiance'].shape[0]
                end_idx = start_idx + batch_size
                
                # Load parameters from individual datasets
                for i, param_name in enumerate(self.parameter_names):
                    all_parameters[start_idx:end_idx, i] = f[f'params/{param_name}'][:]

                all_irradiance[start_idx:end_idx] = f['irradiance'][:]
                all_field_real[start_idx:end_idx] = f['field_real'][:]
                all_field_imag[start_idx:end_idx] = f['field_imag'][:]
                
                start_idx = end_idx
        
        return all_parameters, all_irradiance, all_field_real, all_field_imag
    
    def load_sample_batch(self, batch_size: int = 32) -> Iterator[Tuple[np.ndarray, np.ndarray]]:
        """
        Generator for loading data in batches.
        
        Args:
            batch_size: Size of each batch
            
        Yields:
            Tuples of (parameters, targets) where targets include irradiance and fields
        """
        for batch_file in self.batch_files:
            with h5py.File(batch_file, 'r') as f:
                file_samples = f['irradiance'].shape[0]
                
                for start_idx in range(0, file_samples, batch_size):
                    end_idx = min(start_idx + batch_size, file_samples)
                    
                    # Load parameters from individual datasets and stack them
                    current_batch_params = []
                    for param_name in self.parameter_names:
                        current_batch_params.append(f[f'params/{param_name}'][start_idx:end_idx])
                    parameters = np.stack(current_batch_params, axis=1)
                    
                    # Load targets (combine real and imaginary parts)
                    field_real = f['field_real'][start_idx:end_idx]
                    field_imag = f['field_imag'][start_idx:end_idx]
                    targets = np.stack([field_real, field_imag], axis=1)  # Shape: (batch, 2, H, W)
                    
                    yield parameters, targets
    
    def create_dataloaders(
        self,
        batch_size: int = 32,
        validation_split: float = 0.2,
        num_workers: int = 4,
        shuffle: bool = True
    ) -> Tuple[DataLoader, DataLoader]:
        """
        Create PyTorch DataLoaders for training and validation.
        
        Args:
            batch_size: Batch size
            validation_split: Fraction of data for validation
            num_workers: Number of data loading workers
            shuffle: Whether to shuffle data
            
        Returns:
            Tuple of (train_loader, val_loader)
        """
        # Load all data
        parameters, irradiance, field_real, field_imag = self.load_all_data()
        
        # Combine real and imaginary parts
        targets = np.stack([field_real, field_imag], axis=1)  # Shape: (N, 2, H, W)
        
        # Create train/validation split
        num_samples = len(parameters)
        indices = np.arange(num_samples)
        if shuffle:
            np.random.shuffle(indices)
        
        split_idx = int(num_samples * (1 - validation_split))
        train_indices = indices[:split_idx]
        val_indices = indices[split_idx:]
        
        # Create datasets
        train_dataset = FSOCDataset(parameters[train_indices], targets[train_indices])
        val_dataset = FSOCDataset(parameters[val_indices], targets[val_indices])
        
        # Create data loaders
        train_loader = DataLoader(
            train_dataset,
            batch_size=batch_size,
            shuffle=shuffle,
            num_workers=num_workers,
            pin_memory=torch.cuda.is_available()
        )
        
        val_loader = DataLoader(
            val_dataset,
            batch_size=batch_size,
            shuffle=False,
            num_workers=num_workers,
            pin_memory=torch.cuda.is_available()
        )
        
        return train_loader, val_loader
    
    def load_test_data(self, num_samples: Optional[int] = None) -> Tuple[np.ndarray, np.ndarray]:
        """
        Load test data (parameters and targets).
        
        Args:
            num_samples: Number of samples to load (None for all)
            
        Returns:
            Tuple of (parameters, targets)
        """
        parameters, irradiance, field_real, field_imag = self.load_all_data()
        targets = np.stack([field_real, field_imag], axis=1)
        
        if num_samples is not None:
            parameters = parameters[:num_samples]
            targets = targets[:num_samples]
        
        return parameters, targets
    
    def compute_statistics(self) -> Dict:
        """
        Compute dataset statistics.
        
        Returns:
            Dictionary with parameter and target statistics
        """
        parameters, irradiance, field_real, field_imag = self.load_all_data()
        
        # Parameter statistics
        param_stats = {}
        param_names = self.parameter_names
        
        for i, name in enumerate(param_names):
            param_stats[name] = {
                'mean': float(np.mean(parameters[:, i])),
                'std': float(np.std(parameters[:, i])),
                'min': float(np.min(parameters[:, i])),
                'max': float(np.max(parameters[:, i]))
            }
        
        # Target statistics
        target_stats = {
            'irradiance': {
                'mean': float(np.mean(irradiance)),
                'std': float(np.std(irradiance)),
                'min': float(np.min(irradiance)),
                'max': float(np.max(irradiance))
            },
            'field_real': {
                'mean': float(np.mean(field_real)),
                'std': float(np.std(field_real)),
                'min': float(np.min(field_real)),
                'max': float(np.max(field_real))
            },
            'field_imag': {
                'mean': float(np.mean(field_imag)),
                'std': float(np.std(field_imag)),
                'min': float(np.min(field_imag)),
                'max': float(np.max(field_imag))
            }
        }
        
        return {
            'parameters': param_stats,
            'targets': target_stats,
            'dataset_info': self.get_dataset_info()
        }


class FSOCDataset(Dataset):
    """
    PyTorch Dataset for FSOC simulation data.
    """
    
    def __init__(self, parameters: np.ndarray, targets: np.ndarray):
        """
        Initialize dataset.
        
        Args:
            parameters: Input parameters array of shape (N, num_params)
            targets: Target fields array of shape (N, 2, H, W)
        """
        self.parameters = torch.from_numpy(parameters).float()
        self.targets = torch.from_numpy(targets).float()
    
    def __len__(self) -> int:
        return len(self.parameters)
    
    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:
        return self.parameters[idx], self.targets[idx]
    
    def get_parameter_stats(self) -> Dict:
        """Get parameter statistics."""
        params_np = self.parameters.numpy()
        return {
            'mean': np.mean(params_np, axis=0),
            'std': np.std(params_np, axis=0),
            'min': np.min(params_np, axis=0),
            'max': np.max(params_np, axis=0)
        }
    
    def normalize_parameters(self, mean: Optional[np.ndarray] = None, std: Optional[np.ndarray] = None):
        """
        Normalize parameters to zero mean and unit variance.
        
        Args:
            mean: Parameter means (computed if None)
            std: Parameter standard deviations (computed if None)
        """
        if mean is None or std is None:
            stats = self.get_parameter_stats()
            mean = stats['mean']
            std = stats['std']
        
        self.parameters = (self.parameters - torch.from_numpy(mean).float()) / torch.from_numpy(std).float()
        
        return mean, std
