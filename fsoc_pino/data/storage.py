"""
HDF5 data storage and management for FSOC-PINO datasets.

This module provides utilities for loading, processing, and managing
HDF5 datasets generated by the simulation pipeline.
"""

import numpy as np
from typing import List, Dict, Tuple, Optional, Iterator
from pathlib import Path
import h5py
import json
import torch
from torch.utils.data import Dataset, DataLoader


class HDF5Manager:
    """
    Manager for HDF5 dataset files.
    
    Provides utilities for loading, combining, and processing
    multiple HDF5 dataset files.
    """
    
    def __init__(self, dataset_dir: Path):
        """
        Initialize HDF5 manager.
        
        Args:
            dataset_dir: Directory containing HDF5 dataset files
        """
        self.dataset_dir = Path(dataset_dir)
        self.batch_files = list(self.dataset_dir.glob("batch_*.h5"))
        self.metadata_file = self.dataset_dir / "dataset_metadata.json"
        
        if not self.batch_files:
            raise ValueError(f"No batch files found in {dataset_dir}")
        
        # Load metadata
        self.metadata = self._load_metadata()
        
        # Cache dataset info
        self._cache_dataset_info()
    
    def _load_metadata(self) -> Dict:
        """Load dataset metadata."""
        if self.metadata_file.exists():
            with open(self.metadata_file, 'r') as f:
                return json.load(f)
        else:
            # Create minimal metadata from first file by reading attributes
            with h5py.File(self.batch_files[0], 'r') as f:
                return {
                    'parameter_names': list(f.attrs['parameter_names']),
                    'grid_size': int(f.attrs['grid_size'])
                }
    
    def _cache_dataset_info(self):
        """Cache dataset information."""
        self.total_samples = 0
        self.grid_size = None
        self.num_parameters = None
        self.parameter_names = []
        
        for batch_file in self.batch_files:
            with h5py.File(batch_file, 'r') as f:
                self.total_samples += f['irradiance'].shape[0]
                if self.grid_size is None:
                    self.grid_size = f['irradiance'].shape[1]
                    self.parameter_names = list(f.attrs['parameter_names'])
                    self.num_parameters = len(self.parameter_names)
    
    def get_dataset_info(self) -> Dict:
        """Get dataset information."""
        return {
            'total_samples': self.total_samples,
            'grid_size': self.grid_size,
            'num_parameters': self.num_parameters,
            'num_batch_files': len(self.batch_files),
            'parameter_names': self.metadata.get('parameter_names', []),
            'dataset_dir': str(self.dataset_dir)
        }
    
    def load_all_data(self) -> Tuple[np.ndarray, np.ndarray, np.ndarray, np.ndarray]:
        """
        Load all data into memory.
        
        Returns:
            Tuple of (parameters, irradiance, field_real, field_imag)
        """
        # Pre-allocate arrays
        all_parameters = np.zeros((self.total_samples, self.num_parameters), dtype=np.float32)
        all_irradiance = np.zeros((self.total_samples, self.grid_size, self.grid_size), dtype=np.float32)
        all_field_real = np.zeros((self.total_samples, self.grid_size, self.grid_size), dtype=np.float32)
        all_field_imag = np.zeros((self.total_samples, self.grid_size, self.grid_size), dtype=np.float32)
        
        # Load data from all batch files
        start_idx = 0
        for batch_file in self.batch_files:
            with h5py.File(batch_file, 'r') as f:
                batch_size = f['irradiance'].shape[0]
                end_idx = start_idx + batch_size
                
                # Load parameters from individual datasets
                for i, param_name in enumerate(self.parameter_names):
                    all_parameters[start_idx:end_idx, i] = f[f'params/{param_name}'][:]

                all_irradiance[start_idx:end_idx] = f['irradiance'][:]
                all_field_real[start_idx:end_idx] = f['field_real'][:]
                all_field_imag[start_idx:end_idx] = f['field_imag'][:]
                
                start_idx = end_idx
        
        return all_parameters, all_irradiance, all_field_real, all_field_imag
    
    def load_sample_batch(self, batch_size: int = 32) -> Iterator[Tuple[np.ndarray, np.ndarray]]:
        """
        Generator for loading data in batches.
        
        Args:
            batch_size: Size of each batch
            
        Yields:
            Tuples of (parameters, targets) where targets include irradiance and fields
        """
        for batch_file in self.batch_files:
            with h5py.File(batch_file, 'r') as f:
                file_samples = f['irradiance'].shape[0]
                
                for start_idx in range(0, file_samples, batch_size):
                    end_idx = min(start_idx + batch_size, file_samples)
                    
                    # Load parameters from individual datasets and stack them
                    current_batch_params = []
                    for param_name in self.parameter_names:
                        current_batch_params.append(f[f'params/{param_name}'][start_idx:end_idx])
                    parameters = np.stack(current_batch_params, axis=1)
                    
                    # Load targets (combine real and imaginary parts)
                    field_real = f['field_real'][start_idx:end_idx]
                    field_imag = f['field_imag'][start_idx:end_idx]
                    targets = np.stack([field_real, field_imag], axis=1)  # Shape: (batch, 2, H, W)
                    
                    yield parameters, targets
    
    def create_dataloaders(
        self,
        batch_size: int = 32,
        validation_split: float = 0.2,
        num_workers: int = 4,
        shuffle: bool = True
    ) -> Tuple[DataLoader, DataLoader]:
        """
        Create PyTorch DataLoaders for training and validation.
        
        Args:
            batch_size: Batch size
            validation_split: Fraction of data for validation
            num_workers: Number of data loading workers
            shuffle: Whether to shuffle data
            
        Returns:
            Tuple of (train_loader, val_loader)
        """
        num_samples = self.total_samples
        indices = np.arange(num_samples)
        if shuffle:
            np.random.shuffle(indices)
        
        split_idx = int(num_samples * (1 - validation_split))
        train_indices = indices[:split_idx]
        val_indices = indices[split_idx:]
        
        # Create datasets using the HDF5Manager instance
        train_dataset = FSOCDataset(self, train_indices)
        val_dataset = FSOCDataset(self, val_indices)
        
        # Create data loaders
        train_loader = DataLoader(
            train_dataset,
            batch_size=batch_size,
            shuffle=shuffle,
            num_workers=num_workers,
            pin_memory=torch.cuda.is_available()
        )
        
        val_loader = DataLoader(
            val_dataset,
            batch_size=batch_size,
            shuffle=False,
            num_workers=num_workers,
            pin_memory=torch.cuda.is_available()
        )
        
        return train_loader, val_loader
    
    def load_test_data(self, num_samples: Optional[int] = None) -> Tuple[np.ndarray, np.ndarray]:
        """
        Load test data (parameters and targets).
        
        Args:
            num_samples: Number of samples to load (None for all)
            
        Returns:
            Tuple of (parameters, targets) as numpy arrays
        """
        # Create a dataset for the test data
        test_dataset = FSOCDataset(self)
        
        if num_samples is not None:
            # If a specific number of samples is requested, take a subset of indices
            indices = np.arange(min(num_samples, test_dataset.total_samples))
            test_dataset.indices = indices
            test_dataset.total_samples = len(indices)

        # Load data using a DataLoader to handle batching if needed
        test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False, num_workers=0) # Use 0 workers for simplicity in test data loading

        all_params = []
        all_targets = []
        for params, targets in test_loader:
            all_params.append(params.cpu().numpy())
            all_targets.append(targets.cpu().numpy())
        
        return np.concatenate(all_params, axis=0), np.concatenate(all_targets, axis=0)
    
    def compute_statistics(self) -> Dict:
        """
        Compute dataset statistics.
        
        Returns:
            Dictionary with parameter and target statistics
        """
        parameters, irradiance, field_real, field_imag = self.load_all_data()
        
        # Parameter statistics
        param_stats = {}
        param_names = self.parameter_names
        
        for i, name in enumerate(param_names):
            param_stats[name] = {
                'mean': float(np.mean(parameters[:, i])),
                'std': float(np.std(parameters[:, i])),
                'min': float(np.min(parameters[:, i])),
                'max': float(np.max(parameters[:, i]))
            }
        
        # Target statistics
        target_stats = {
            'irradiance': {
                'mean': float(np.mean(irradiance)),
                'std': float(np.std(irradiance)),
                'min': float(np.min(irradiance)),
                'max': float(np.max(irradiance))
            },
            'field_real': {
                'mean': float(np.mean(field_real)),
                'std': float(np.std(field_real)),
                'min': float(np.min(field_real)),
                'max': float(np.max(field_real))
            },
            'field_imag': {
                'mean': float(np.mean(field_imag)),
                'std': float(np.std(field_imag)),
                'min': float(np.min(field_imag)),
                'max': float(np.max(field_imag))
            }
        }
        
        return {
            'parameters': param_stats,
            'targets': target_stats,
            'dataset_info': self.get_dataset_info()
        }


class FSOCDataset(Dataset):
    """
    PyTorch Dataset for FSOC simulation data.
    Loads data directly from HDF5 files in a memory-efficient manner.
    """
    
    def __init__(self, hdf5_manager: HDF5Manager, indices: Optional[np.ndarray] = None):
        """
        Initialize dataset.
        
        Args:
            hdf5_manager: An instance of HDF5Manager to access data files.
            indices: Optional array of indices to load a subset of the data (for train/val split).
        """
        self.hdf5_manager = hdf5_manager
        self.indices = indices if indices is not None else np.arange(self.hdf5_manager.total_samples)
        self.total_samples = len(self.indices)
        
        # Cache parameter names for consistent ordering
        self.parameter_names = self.hdf5_manager.parameter_names

    def __len__(self) -> int:
        return self.total_samples
    
    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor]:
        # Map the requested index to the global index in the HDF5 files
        global_idx = self.indices[idx]

        # Determine which batch file and index within that file the sample belongs to
        # This requires knowing the cumulative sum of samples in each batch file.
        # For simplicity, we'll iterate through files. For very large numbers of files,
        # a more optimized lookup (e.g., binary search on cumulative sums) would be better.

        current_sample_count = 0
        for batch_file in self.hdf5_manager.batch_files:
            with h5py.File(batch_file, 'r') as f:
                file_num_samples = f['irradiance'].shape[0]

                if global_idx < current_sample_count + file_num_samples:
                    # Sample is in this file
                    idx_in_file = global_idx - current_sample_count

                    # Load parameters
                    params_list = []
                    for param_name in self.parameter_names:
                        params_list.append(f[f'params/{param_name}'][idx_in_file])
                    parameters = np.array(params_list, dtype=np.float32)

                    # Load targets
                    field_real = f['field_real'][idx_in_file]
                    field_imag = f['field_imag'][idx_in_file]
                    targets = np.stack([field_real, field_imag], axis=0) # Shape: (2, H, W)

                    # Convert to tensors
                    parameters_tensor = torch.from_numpy(parameters).float()
                    targets_tensor = torch.from_numpy(targets).float()

                    # Apply parameter normalization if set
                    parameters_tensor = self._get_normalized_parameters(parameters_tensor)

                    return parameters_tensor, targets_tensor

                current_sample_count += file_num_samples

        raise IndexError(f"Sample index {global_idx} out of bounds.")
    
    def get_parameter_stats(self) -> Dict:
        """Compute parameter statistics by loading all parameters (can be memory intensive)."""
        # This method still loads all parameters to compute stats. For extremely large datasets,
        # this would need to be done via a streaming approach or pre-computed and stored in metadata.
        all_params = []
        for batch_file in self.hdf5_manager.batch_files:
            with h5py.File(batch_file, 'r') as f:
                batch_params = []
                for param_name in self.parameter_names:
                    batch_params.append(f[f'params/{param_name}'][:])
                all_params.append(np.stack(batch_params, axis=1))
        
        params_np = np.concatenate(all_params, axis=0)

        return {
            'mean': np.mean(params_np, axis=0),
            'std': np.std(params_np, axis=0),
            'min': np.min(params_np, axis=0),
            'max': np.max(params_np, axis=0)
        }
    
    def normalize_parameters(self, mean: Optional[np.ndarray] = None, std: Optional[np.ndarray] = None):
        """
        Normalize parameters to zero mean and unit variance.
        
        This method modifies the behavior of __getitem__ to return normalized parameters.
        The normalization statistics are stored within the dataset instance.
        """
        if mean is None or std is None:
            stats = self.get_parameter_stats()
            self._param_mean = torch.from_numpy(stats['mean']).float()
            self._param_std = torch.from_numpy(stats['std']).float()
        else:
            self._param_mean = torch.from_numpy(mean).float()
            self._param_std = torch.from_numpy(std).float()
        
        self._normalize_params_on_get = True

    def _get_normalized_parameters(self, parameters: torch.Tensor) -> torch.Tensor:
        if hasattr(self, '_normalize_params_on_get') and self._normalize_params_on_get:
            return (parameters - self._param_mean.to(parameters.device)) / (self._param_std.to(parameters.device) + 1e-8)
        return parameters


